{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "07aa7d23",
   "metadata": {},
   "source": [
    "<h1 style=\"font-size:30px;\">Application: Social Distancing Monitor</h1>\n",
    "\n",
    "In a world hit by a Pandemic, it is important to be careful and take precautions for our own as well as the greater good. A few ways to stay safe include getting vaccinated if you can, wearing a mask, and keeping a Social Distance of 6ft (or two meters). In this notebook, we will be implementing a Social Distancing Monitor using OpenCV's DNN Module and a MobileNet-SSD object detection model.\n",
    "\n",
    "Consider the following frame from a video feed. Our goal is to understand if, and who, is unsafely close together. Try running the python script accompanying this notebook to try it out for yourself.\n",
    "\n",
    "<br>\n",
    "<center>\n",
    "<img src=\"https://opencv.org/wp-content/uploads/2021/09/c0-m15-input-frame.png\" alt=\"Waiting Room\">\n",
    "</center>\n",
    "<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c097e00",
   "metadata": {},
   "outputs": [],
   "source": [
    "if 'google.colab' in str(get_ipython()):\n",
    "    print(\"Downloading Code to Colab Environment\")\n",
    "    !wget https://www.dropbox.com/sh/uro596fmm67in3b/AABurDoQj5tS94EgUDQXkcBaa?dl=1 -O module-code.zip -q --show-progress\n",
    "    !unzip -qq module-code.zip\n",
    "    !pip install --upgrade opencv-contrib-python\n",
    "    %cd Applications/\n",
    "else:\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43194f89",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54a571b6",
   "metadata": {},
   "source": [
    "# 1. Load MobilenetSSD Model\n",
    "\n",
    "The model is obtained from the following repo:\n",
    "https://github.com/chuanqi305/MobileNet-SSD\n",
    "\n",
    "The model is trained on the Pascal VOC 2012 data, but it was pre-trained on the COCO dataset. Thus, it gives very good accuracy as well as it performs very fast. This model was trained using the Caffe framework.\n",
    "\n",
    "We load the MobilenetSSD model using the OpenCV's DNN Module. This function takes 2 arguments:\n",
    "1. The `MobileNetSSD_deploy.prototxt` protobuf text file\n",
    "2. The `MobileNetSSD_deploy.caffemodel` binary file\n",
    "\n",
    "This function returns the loaded model object."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd3db1f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "configFile = 'MobileNetSSD_deploy.prototxt'\n",
    "modelFile = 'MobileNetSSD_deploy.caffemodel'\n",
    "net = cv2.dnn.readNetFromCaffe(configFile, modelFile)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "471d8802",
   "metadata": {},
   "source": [
    "# 2. Get Detections\n",
    "We now get the detections from the loaded MobilenetSSD model. We start with finding the detection from the layer outputs obtained from the MobilenetSSD network. We then check if the detection is a person. If yes, we update the bounding box list and the centroids list.\n",
    "\n",
    "This function takes 2 inputs:\n",
    "1. The particular frame\n",
    "2. The MobilenetSSD Pretrained Network\n",
    "\n",
    "After processing, it returns a tuple with the following:\n",
    "1. Confidence\n",
    "2. Bounding box values\n",
    "3. Centroid values\n",
    "\n",
    "As always, we need to use `blobFromImage` with settings matching the training data used to create the model. In this case, we can find this in the `train.prototxt` file from the [github repository](https://github.com/chuanqi305/MobileNet-SSD/blob/master/train.prototxt).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fff169e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def detect(frame, network):\n",
    "    \"\"\"Detects any humans and returns their bounding box and centers.\"\"\"\n",
    "    results = []\n",
    "    h, w = frame.shape[:2]\n",
    "\n",
    "    # Pre-processing: mean subtraction and scaling to match model's training set.\n",
    "    blob = cv2.dnn.blobFromImage(frame, 0.007843, (300, 300), [127.5, 127.5, 127.5])\n",
    "    network.setInput(blob)\n",
    "\n",
    "    # Run an inference of the model, passing blob through the network.\n",
    "    network_output = network.forward()\n",
    "\n",
    "    # Loop over all results.\n",
    "    for i in np.arange(0, network_output.shape[2]):\n",
    "        class_id = network_output[0, 0, i, 1]\n",
    "        confidence = network_output[0, 0, i, 2]\n",
    "\n",
    "        # Filter for only detected people (classID 15) and high confidence.\n",
    "        # https://github.com/chuanqi305/MobileNet-SSD/blob/master/demo.py#L21\n",
    "        if confidence > 0.7 and class_id == 15:\n",
    "            # Remap 0-1 position outputs to size of image for bounding box.\n",
    "            box = network_output[0, 0, i, 3:7] * np.array([w, h, w, h])\n",
    "            box = box.astype('int')\n",
    "\n",
    "            # Calculate the person center from the bounding box.\n",
    "            center_x = int((box[0] + box[2]) / 2)\n",
    "            center_y = int((box[1] + box[3]) / 2)\n",
    "\n",
    "            results.append((confidence, box, (center_x, center_y)))\n",
    "    return results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dffa1935",
   "metadata": {},
   "source": [
    "# 3. Detect Violations from Set of Detections\n",
    "For detecting violations, we calculate the distance matrix for each pair of detection and then update the distance threshold for that pair based as 1.2 times the pixel width of the smallest detection. If the pair violates social distancing as per the distance threshold, it is marked as a violation. \n",
    "\n",
    "This function takes an input, which is the result of the frame passed through the neural network, and returns the violations set as the output.\n",
    "\n",
    "You can tune the multiplying factor to suit your application."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61a05f82",
   "metadata": {},
   "outputs": [],
   "source": [
    "def detect_violations(results):\n",
    "    \"\"\"Detects if there are any people who are unsafely close together.\"\"\"\n",
    "    violations = set()\n",
    "    # Multiplier on the pixel width of the smallest detection.\n",
    "    fac = 1.2\n",
    "\n",
    "    if len(results) >= 2:\n",
    "        # Width is right edge minus left.\n",
    "        boxes_width = np.array([abs(int(r[1][2] - r[1][0])) for r in results])\n",
    "        centroids = np.array([r[2] for r in results])\n",
    "        distance_matrix = euclidean_dist(centroids, centroids)\n",
    "        \n",
    "        # For each starting detection...\n",
    "        for row in range(distance_matrix.shape[0]):\n",
    "            # Compare distance with every other remaining detection.\n",
    "            for col in range(row + 1, distance_matrix.shape[1]):\n",
    "                # Presume unsafe if closer than 1.2x (fac) width of a person apart.\n",
    "                ref_distance = int(fac * min(boxes_width[row], boxes_width[col]))\n",
    "\n",
    "                if distance_matrix[row, col] < ref_distance:\n",
    "                    violations.add(row)\n",
    "                    violations.add(col)\n",
    "    return violations"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c408df92",
   "metadata": {},
   "source": [
    "# 4. Calculate Distance Matrix\n",
    "We use the euclidean formula to calculate distance i.e., \n",
    "\n",
    "$ Distance = \\sqrt{(x_{A}-x_{B}) ^{2} + (y_{A}-y_{B}) ^{2}} $ \n",
    "\n",
    "This function takes 2 numpy arrays as the input and returns the Distance matrix as the output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "426d24ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "def euclidean_dist(A, B):\n",
    "    \"\"\"Calculates pair-wise distance between each centroid combination.\n",
    "\n",
    "    Returns a matrix of len(A) by len(B).\"\"\"\n",
    "    p1 = np.sum(A**2, axis=1)[:, np.newaxis]\n",
    "    p2 = np.sum(B**2, axis=1)\n",
    "    p3 = -2 * np.dot(A, B.T)\n",
    "    return np.round(np.sqrt(p1 + p2 + p3), 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc9fd0cd",
   "metadata": {},
   "source": [
    "# 5. Write Video to File Directory\n",
    "To write video to the file directory, we use the `cv2.VideoWriter()` function. This function writes a video file frame by frame to the file directory. \n",
    "\n",
    "```python\n",
    "fourcc = cv2.VideoWriter_fourcc(*'MP4V')\n",
    "writer = cv2.VideoWriter(OUTPUT_PATH, fourcc, 25, (frame.shape[1], frame.shape[0]), True)\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b648f16d",
   "metadata": {},
   "source": [
    "# 6. Loading and Running model on the Video\n",
    "We have to now load the required video file and run all the above mentioned functions on it. We start will loading the pre-trained Network. We then read the video frame by frame in a While loop and perform the required operations on each frame. This includes getting detections in each frame, detecting violations, drawing a bounding box around each detection and marking them Safe/Unsafe and finally displaying and saving the output video."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e999a80",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "SHOW_VIDEO = 0\n",
    "INPUT_PATH = 'input.mp4'\n",
    "OUTPUT_PATH = 'output.mp4'\n",
    "\n",
    "cap = cv2.VideoCapture(INPUT_PATH)\n",
    "\n",
    "writer = None\n",
    "\n",
    "prev_frame_time = 0\n",
    "new_frame_time = 0\n",
    "counter = 0\n",
    "\n",
    "print(\"Processing frames please wait ...\")\n",
    "\n",
    "while cap.isOpened():\n",
    "    ret, frame = cap.read()\n",
    "\n",
    "    if not ret:\n",
    "        break\n",
    "\n",
    "    # Detect Boxes.\n",
    "    results = detect(frame, network=net)\n",
    "\n",
    "    # Detect boxes too close (i.e. the violations).\n",
    "    violations = detect_violations(results)\n",
    "\n",
    "    t, _ = net.getPerfProfile()\n",
    "    label = 'Inference time: %.2f ms' % (t * 1000.0 / cv2.getTickFrequency())\n",
    "\n",
    "    # Plot all bounding boxes and whether they are in violation\n",
    "    for index, (prob, bounding_box, centroid) in enumerate(results):\n",
    "        start_x, start_y, end_x, end_y = bounding_box\n",
    "\n",
    "        # Color red if violation, otherwise color green.\n",
    "        color = (0, 0, 255) if index in violations else (0, 255, 0)\n",
    "        cv2.rectangle(frame, (start_x, start_y), (end_x, end_y), color, 2)\n",
    "\n",
    "        cv2.putText(\n",
    "            frame, label,\n",
    "            (2, frame.shape[0] - 4),\n",
    "            cv2.FONT_HERSHEY_TRIPLEX, 0.4, (255, 255, 255))\n",
    "        cv2.putText(\n",
    "            frame, 'Not Safe' if index in violations else 'Safe',\n",
    "            (start_x, start_y - 10),\n",
    "            cv2.FONT_HERSHEY_SIMPLEX, 0.6, color, 2)\n",
    "\n",
    "        cv2.putText(\n",
    "            frame, f'Num Violations: {len(violations)}',\n",
    "            (10, frame.shape[0] - 25),\n",
    "            fontFace=cv2.FONT_HERSHEY_PLAIN,\n",
    "            fontScale=1.0, color=(0, 0, 255), thickness=1)\n",
    "\n",
    "    if SHOW_VIDEO:\n",
    "        cv2.imshow('frame', frame)\n",
    "        if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "            break\n",
    "\n",
    "    if writer is None:\n",
    "        fourcc = cv2.VideoWriter_fourcc(*'mp4v')\n",
    "        writer = cv2.VideoWriter(\n",
    "            OUTPUT_PATH, fourcc, 25, (frame.shape[1], frame.shape[0]), True)\n",
    "\n",
    "    if writer:\n",
    "        writer.write(frame)\n",
    "\n",
    "cap.release()\n",
    "writer.release()\n",
    "print(f'Finished Writing Video to {OUTPUT_PATH}')\n",
    "cv2.destroyAllWindows()\n",
    "print('Cleared all windows...')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32c36b99",
   "metadata": {},
   "source": [
    "# 7. View the results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "227016db",
   "metadata": {},
   "source": [
    "See below the resulting output of the analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "545b90ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "from moviepy import VideoFileClip\n",
    "clip = VideoFileClip(\"./output.mp4\")\n",
    "clip.display_in_notebook(width=800)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "opencv-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
