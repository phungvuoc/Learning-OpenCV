{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Introduction to Depth-Aware Image Processing with OpenCV\n",
        "\n",
        "You've already mastered OpenCV and understood it's versatility in image applications.  Now, we're going to take things a step further and gently dip our toes into the fascinating world of **depth**.\n",
        "\n",
        "\n",
        "<div style=\"text-align: center;\">\n",
        "  <figure style=\"display: inline-block;\">\n",
        "    <img src=\"https://learnopencv.com/wp-content/uploads/2024/02/animal-depth-anything-large.gif\" alt=\"Monocular Depth Estimation\" width = 900>\n",
        "    <figcaption style=\"text-align: center;\">Monocular Depth Estimation</figcaption>\n",
        "  </figure>\n",
        "</div>\n",
        "\n",
        "\n",
        "### What is Depth Estimation?  \n",
        "\n",
        "**Depth Estimation** is the process of determining how far objects are from the camera in a given scene.  \n",
        "\n",
        "**Think of it this way:**  In everyday life, our eyes naturally perceive depth, allowing us to distinguish between objects that are near and those that are farther away. However, when a camera captures an image, it flattens the scene into two dimensions, losing this depth information. Depth estimation aims to reconstruct this missing third dimension by analyzing visual cues from a single image, multiple images, or specialized sensors, helping computers interpret spatial relationships just as we do.\n",
        "\n"
      ],
      "metadata": {
        "id": "qZB5P72PZqZf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### How Do We Get Depth Maps?\n",
        "\n",
        "There are several approaches to generate depth maps:\n",
        "\n",
        "* **Hardware-Based Depth Sensors (Active Sensing)**: These sensors actively measure depth by emitting light (like infrared) and calculating the time it takes for the light to bounce back, providing direct depth measurements. Examples include Time-of-Flight(ToF) sensors and LiDAR.\n",
        "\n",
        "  > **Did you know?** Many modern iPhones, starting from the iPhone 12 Pro series, are equipped with LiDAR scanners. This technology enhances augmented reality (AR) experiences by enabling precise depth measurement, allowing developers to create immersive applications using Apple's ARKit framework.\n",
        "  Give a [read here](https://www.opencv.ai/blog/depth-estimation)\n",
        "\n",
        "\n",
        "* **Stereo Vision (Passive Sensing)**: This method uses two cameras, like our eyes! By comparing the slight differences between the two camera images (called \"disparity\" - how much objects shift position), we can calculate depth. It's like our brain figuring out depth from our two eyes. The two sensors have to be kept at a fixed distance apart and must be precisely calibrated to ensure accurate depth estimation.\n",
        "\n",
        "    **For example**: OAK-D or Realsense\n",
        "\n",
        "\n",
        "\n",
        "<div style=\"text-align: center;\">\n",
        "  <figure style=\"display: inline-block;\">\n",
        "    <img src = \"https://learnopencv.com/wp-content/uploads/2021/12/Demostration.gif\" alt=\"Stereo Vision OAK-D\" width = 900>\n",
        "    <figcaption style=\"text-align: center;\">\n",
        "    <a href=\"https://learnopencv.com/category/oak/\" target=\"_blank\">OAK-D Stereo Series</a>\n",
        "    </figcaption>\n",
        "  </figure>\n",
        "</div>\n",
        "\n",
        "        \n",
        "\n",
        "*   **Monocular Depth Estimation (Deep Learning!):** Impressively, deep learning algorithms enable depth estimation from a **single** 2D image.  These models learn to understand depth cues inherent in a 2D image.  By training on vast datasets of images with corresponding depth information, these models can infer the depth of objects in a new image even without direct depth measurement. Examples of such models are [Depth Anything](https://learnopencv.com/depth-anything/) and [Depth Pro](https://learnopencv.com/depth-pro-monocular-metric-depth/), representing state-of-the-art monocular depth estimation capabilities.\n",
        "\n"
      ],
      "metadata": {
        "id": "lJTwabR88v9-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Real-World Applications of Depth Maps:\n",
        "\n",
        "<div style=\"text-align: center;\">\n",
        "  <figure style=\"display: inline-block;\">\n",
        "    <video\n",
        "       controls\n",
        "       src=\"https://www.dropbox.com/scl/fi/xq79eokv4biumf3hm2cws/opencv_university_dust3r_3d.mp4?rlkey=lstblu15mbhtktzffri2wkpfi&st=idj18h4w&dl=1\"\n",
        "       width=\"640\">\n",
        "    </video>\n",
        "    <figcaption>OpenCV University Office 3D Reconstruction</figcaption>\n",
        "  </figure>\n",
        "</div>\n",
        "\n",
        "\n",
        "\n",
        "- **Object Scanning & 3D Reconstruction**: Niantic, known for games like Pok√©mon Go, has built a comprehensive 3D map of the world using depth data. This map enhances augmented reality experiences by allowing virtual objects to interact seamlessly with real-world environments. [[1]](https://nianticlabs.com/news/manydepth-research?hl=en)\n",
        "\n",
        "- **Measurement & Industrial Use**: DEPTHX, an autonomous underwater vehicle, utilized depth mapping to explore and create 3D models of underwater sinkholes in Mexico. This technology enabled precise mapping and analysis of previously inaccessible environments. ‚Äã[[2]](http://stoneaerospace.com/depthx/)\n",
        "\n",
        "- **Augmented & Virtual Reality**: Google's ARCore includes a robust Depth API that uses machine learning to infer depth from a single camera view. This allows developers to implement realistic occlusion, accurate environmental mapping, and precise object placement, resulting in highly immersive augmented reality experiences on Android devices.[[3]](https://developers.google.com/ar/develop/depth)\n",
        "\n",
        "- **Photography & Videography**: Modern smartphones use depth maps to simulate shallow depth-of-field effects, allowing users to capture photos with artistically blurred backgrounds, mimicking the capabilities of professional cameras. ‚Äã[[4]](https://research.google/pubs/synthetic-depth-of-field-with-a-single-camera-mobile-phone/)\n",
        "\n",
        "- **Autonomous Systems & Robotics**: Toyota Research Institute developed a self-supervised monocular depth estimation framework to improve the perception capabilities of autonomous vehicles, enhancing their ability to navigate complex environments safely. ‚Äã[[5]](https://medium.com/toyotaresearch/monocular-depth-in-the-real-world-99c2b287df34)\n",
        "\n",
        "- **Gaming & Entertainment**: The Kinect Developer Kit harnesses advanced depth-sensing technology to capture a real-time 3D map of its surroundings. Using an infrared sensor‚Äîwith structured light in the original Kinect and time-of-flight in Kinect v2‚Äîit generates detailed point clouds that enable accurate body tracking and gesture recognition. This technology powered games like Kinect Sports, where players could control actions with natural movements, and has been used in interactive installations where user motion directly influences digital environments. [[6]](https://azure.microsoft.com/en-in/products/kinect-dk)\n",
        "\n",
        "and more....\n",
        "\n",
        "\n",
        "\n",
        "<div style=\"text-align: center;\">\n",
        "  <figure style=\"display: inline-block;\">\n",
        "    <video\n",
        "       controls\n",
        "       src=\"https://www.mavdrones.com/wp-content/uploads/2023/07/dc85bcf9-6382-4a93-969b-365a759ba1cf.mp4\"\n",
        "       width=\"640\">\n",
        "    </video>\n",
        "    <figcaption><b>Distance Measurement with Depth</b> - DJI Zenmuse L1 Lidar</figcaption>\n",
        "  </figure>\n",
        "</div>\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "r3PQRMFH2IPL"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "### üìå Our Approach in This Module  \n",
        "- We **do not estimate depth using any depth-sensing cameras** (e.g., stereo vision or LiDAR).  \n",
        "- Instead, we use **monocular depth estimation** from a **pretrained deep learning model like Depth Pro** to generate a depth map.  \n",
        "- Then, we apply **OpenCV techniques** to create **depth-aware effects**.\n",
        "\n",
        "\n",
        "\n",
        "We‚Äôre keeping it simple‚Äîno deep dives into the nitty-gritty of depth estimation. Just the essentials needed for our application.\n",
        "\n",
        "**Disclaimer:** This module is a beginner-friendly introduction to using depth maps. We‚Äôre not getting into the complex math or deep learning specifics behind depth estimation. Instead, we‚Äôll focus on how to take a depth map (predicted by a deep learning model) and use OpenCV to create depth-aware effects.\n",
        "\n",
        "\"Don‚Äôt worry if you don‚Äôt fully understand\" depth estimation or Depth Pro. If you get the basic idea of how we humans perceive depth, you‚Äôre good to go. Feel free to jump straight to the applications section!\n"
      ],
      "metadata": {
        "id": "P6yba1OD2eBi"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### How Monocular Depth Works  \n",
        "These models are **trained on large datasets** of images with corresponding depth information. By recognizing and interpreting depth cues, the model learns to **infer depth from 2D images** without requiring stereo cameras or additional sensors.\n",
        "\n",
        "**üîπ Depth Cues Used by the Model:**  \n",
        "- **Perspective** ‚Üí Objects appear smaller as they recede into the distance, and parallel lines seem to converge (e.g., railway tracks meeting at the horizon).\n",
        "- **Texture Gradients** ‚Üí Textures become denser and finer as they move further away, helping the model differentiate between near and far surfaces.  \n",
        "- **Relative Object Size** ‚Üí  When two objects of the same type are present, the one appearing smaller in the image is likely farther away. This assumption works because the model learns from datasets where object sizes are consistent in real-world scenes.\n",
        "- **Focus & Blur** ‚Üí Objects at different depths often appear sharper or blurrier depending on their distance from the camera. The model leverages this information to estimate relative depth.\n",
        "- **Contextual Understanding** ‚Üí The model learns common scene layouts, such as how the sky is always above the ground or how buildings typically rise vertically, to make depth predictions more accurate.\n",
        "\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "#### Output: The Depth Map  \n",
        "Once trained, these models take a **new 2D image** as input and **infer depth at each pixel**, producing a **depth map**.  \n",
        "\n",
        "<div style=\"text-align: center;\">\n",
        "  <figure style=\"display: inline-block;\">\n",
        "    <img src=\"https://www.dropbox.com/scl/fi/ivt704ao003irkivfdhso/Monocular-Depth-Satya.png?rlkey=g5vnwodi51b3db6a2msycfpcc&st=d61f3jsh&dl=1\" alt=\"Monocular Depth Estimation\" width = 900>\n",
        "    <figcaption style=\"text-align: center;\">Relative Monocular Depth Estimation</figcaption>\n",
        "  </figure>\n",
        "</div>\n",
        "\n",
        "üîπ **Depth maps are grayscale images** where:  \n",
        "- **Dark regions** = Objects **closer** to the camera  \n",
        "- **Bright regions** = Objects **farther away**  \n",
        "\n",
        "\n",
        "For visualization purposes, an **inverse depth map** can also be created. In an inverse depth map, the relationship is reversed:\n",
        "\n",
        "- **Bright regions** indicate objects that are closer\n",
        "- **Dark regions** indicate objects that are farther away\n",
        "\n",
        "This transformation is useful because it often enhances the contrast for nearby objects, making them more visually prominent and easier to analyze, especially when the depth range is wide.\n",
        "\n",
        "\n",
        "These depth maps are what we will use in this module to apply **various depth-aware effects** using OpenCV.\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "DCscgeuiF9WD"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "Depth estimation enables **amazing effects** in photography, video, and graphics.  \n",
        "\n",
        "To explore OpenCV-specific algorithms and operations once we obtain depth, we‚Äôll apply them in these three upcoming applications:\n",
        "\n",
        "üñºÔ∏è **A01: Depth Blur or Selective Focus** ‚Üí Controlling which areas remain sharp  \n",
        "\n",
        "üì∏ **A02: Simulating Depth of Field (DoF)** ‚Üí Adjusting focus like a DSLR lens  (***Coming soon...***)\n",
        "\n",
        "üé• **A03: Depth-based Motion Effects** ‚Üí Creating parallax shifts in static images  (***Coming soon...***)\n"
      ],
      "metadata": {
        "id": "5TORa-1IGZ_x"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Related LearnOpenCV Articles:\n",
        "  - [Depth Pro](https://learnopencv.com/depth-pro-monocular-metric-depth/)\n",
        "  - [Depth Estimation](https://learnopencv.com/?s=Depth+Estimation)\n",
        "  - [Stereo Vision](https://learnopencv.com/adas-stereo-vision/)\n",
        "  - [OAK-D](https://learnopencv.com/tag/oak-d/)\n",
        "\n",
        "\n",
        "\n",
        "### Further Reads:\n",
        "\n",
        "1. [Apple AR RoomPlan](https://developer.apple.com/augmented-reality/roomplan/)\n",
        "2. [ARCORE Depth API](https://developers.google.com/ar/develop/depth)\n",
        "3. DJI Drone Video: [Mavdrones](https://www.mavdrones.com/product/call-to-purchase-the-dji-zenmuse-l1-lidar/?srsltid=AfmBOoodoRVtm9bqkZMycRBdWZhoPFC5K96n4aJSvpDPYrLKObmvZyrq)"
      ],
      "metadata": {
        "id": "m0C2OS_1BuB9"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "6QCKawZjZw-i"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}