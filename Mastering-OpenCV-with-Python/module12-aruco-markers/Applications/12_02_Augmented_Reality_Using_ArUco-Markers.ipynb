{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ocbSyzAzXZau"
   },
   "source": [
    "<h1 style=\"font-size:30px;\">Application: Augmented Reality using ArUco Markers</h1> \n",
    "\n",
    "In this application notebook we will demonstrate how to use ArUco Markers to implement augmented reality for images and videos. Four separate use cases will be covered that will allow you to replace a region of interest (ROI) in a Destination image or video with a Source image or video. The ROI in the destination image or video is predefined with the placement of Aruco Markers.\n",
    "\n",
    "<br>\n",
    "<center>\n",
    "<img src=\"https://opencv.org/wp-content/uploads/2021/09/c0-m12-office.jpg\" alt=\"c0-m12-office.jpg\" align=\"middle\">\n",
    "</center>\n",
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "<center>\n",
    "<img src=\"https://opencv.org/wp-content/uploads/2021/09/c0-m12-feature-image-12-02.jpg\" alt=\"c0-m12-feature-image-12-02.jpg\" align=\"middle\">\n",
    "</center>\n",
    "<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from moviepy import *\n",
    "clip = VideoFileClip('office_markers.mp4')\n",
    "clip.display_in_notebook(width = 800)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import mime \n",
    "import mimetypes\n",
    "from IPython.display import Image\n",
    "%matplotlib inline\n",
    "\n",
    "if 'google.colab' in str(get_ipython()):\n",
    "    print(\"Downloading Code to Colab Environment\")\n",
    "    !wget https://www.dropbox.com/sh/zb7oj58ikl0697n/AADkiWODtAbRs6L0g8qxZF7Da?dl=1 -O module-code.zip -q --show-progress\n",
    "    !unzip -qq module-code.zip\n",
    "    %cd Applications/\n",
    "else:\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <font style=\"color:rgb(50,120,230)\">Case 1: Image in Image</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import Image\n",
    "Image('visuals/AR_image_in_image_Apollo-8-launch.jpg')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <font style=\"color:rgb(50,120,230)\">Case 2: Image in Video</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "from moviepy import *\n",
    "clip = VideoFileClip('visuals/AR_image_in_video_New_Zealand_Cove.mp4')\n",
    "clip.display_in_notebook(width = 1000, logger = None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <font style=\"color:rgb(50,120,230)\">Case 3: Video in Image</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clip = VideoFileClip('visuals/AR_video_in_image_boys_playing.mp4')\n",
    "clip.display_in_notebook(width = 1000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <font style=\"color:rgb(50,120,230)\">Case 4: Video in Video</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clip = VideoFileClip('visuals/AR_video_in_video_horse_race.mp4')\n",
    "clip.display_in_notebook(width = 1000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 0. Printing and Placement of ArUco Markers to Define a ROI \n",
    "\n",
    "This proceedure was described in the previous video \"Introducion to ArUco Markers.\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Input Media (setup)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mimetypes.init()\n",
    "\n",
    "case1_img_in_img = ['Apollo-8-launch.png',  'office_markers.jpg']\n",
    "case2_img_in_vid = ['New_Zealand_Cove.jpg', 'office_markers.mp4']\n",
    "case3_vid_in_img = ['boys_playing.mp4',     'office_markers.jpg']\n",
    "case4_vid_in_vid = ['horse_race.mp4',       'office_markers.mp4']\n",
    "\n",
    "case = case4_vid_in_vid\n",
    "\n",
    "marker_ids = [23, 25, 30, 33]\n",
    "\n",
    "# Scale factors used to increase size of source media to cover ArUco Marker borders.\n",
    "scaling_fac_x = .008 \n",
    "scaling_fac_y = .012\n",
    "\n",
    "# Specify the prefix for the output file. The output file media type\n",
    "# will depend on the source and destination media type.\n",
    "prefix = 'AR_'\n",
    "\n",
    "class MediaSpec:\n",
    "    def __init__(self, src, dst):\n",
    "        self.src = src\n",
    "        self.dst = dst\n",
    "        \n",
    "media_spec = MediaSpec(case[0], case[1])\n",
    "\n",
    "# The source may be either an image or video.\n",
    "src_input = media_spec.src\n",
    "\n",
    "# The destination may be either an image or video.\n",
    "dst_input = media_spec.dst\n",
    "\n",
    "# Determine the media types for source and destination.\n",
    "mime_dst = mimetypes.guess_type(dst_input)[0]\n",
    "if mime_dst != None:\n",
    "    mime_dst = mime_dst.split('/')[0]\n",
    "mime_src = mimetypes.guess_type(src_input)[0]\n",
    "if mime_src != None:\n",
    "     mime_src = mime_src.split('/')[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <font style=\"color:rgb(50,120,230)\">Destination Inputs<font/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#------------------------------------------------------------------------------\n",
    "# Destination (image or video). This is the image or video that contains the\n",
    "# original scene without any modification.\n",
    "#------------------------------------------------------------------------------\n",
    "if mime_dst == 'image':\n",
    "    # Read the image.\n",
    "    frame_dst = cv2.imread(dst_input)\n",
    "elif mime_dst == 'video':\n",
    "    # Create a video capture object.\n",
    "    cap_dst = cv2.VideoCapture(dst_input)\n",
    "    fps = cap_dst.get(cv2.CAP_PROP_FPS)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <font style=\"color:rgb(50,120,230)\">Source Inputs<font/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#------------------------------------------------------------------------------\n",
    "# Soure (image of video). This is the image of video that will be transformed\n",
    "# onto the destination image or video.\n",
    "#------------------------------------------------------------------------------\n",
    "if mime_src == 'image':\n",
    "    # Read the image.\n",
    "    frame_src = cv2.imread(src_input)\n",
    "elif mime_src == 'video':\n",
    "    # Create a video capture object.\n",
    "    cap_src = cv2.VideoCapture(src_input)\n",
    "    fps = cap_src.get(cv2.CAP_PROP_FPS)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Specify Output "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# If either source or destination are video, then create a video writer object.\n",
    "if (mime_dst == 'video' or mime_src == 'video'):\n",
    "    \n",
    "    output_file = prefix + str(mime_src) + '_in_' + str(mime_dst) + '_'  + str(src_input[:-4]) + '.mp4'\n",
    "    \n",
    "    if mime_dst == 'video':\n",
    "        # Determine the output video size based on the destination video frame size.\n",
    "        width = round(2 * cap_dst.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
    "        height = round(cap_dst.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
    "    else:\n",
    "        # Determine the output video size based on the destination image frame size.\n",
    "        width = round(2 * frame_dst.shape[1])\n",
    "        height = round(frame_dst.shape[0])\n",
    "        \n",
    "    # Create the video writer object.\n",
    "    video_writer = cv2.VideoWriter(output_file, cv2.VideoWriter_fourcc(*'mp4v'), fps, (width, height))\n",
    "else:\n",
    "    output_file = prefix + 'image_in_image_' + str(src_input[:-4]) + '.jpg'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Define Convenience Functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <font style=\"color:rgb(50,120,230)\">Convenience Function (extract detected points)<font/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract reference point coordinates from marker corners.\n",
    "def extract_pts(marker_ids, ids, corners):\n",
    "\n",
    "    # Upper left corner of ROI.\n",
    "    index = np.squeeze(np.where(ids == marker_ids[0]))\n",
    "    ref_pt1 = np.squeeze(corners[index[0]])[0]\n",
    "\n",
    "    # Upper right corner of ROI.\n",
    "    index = np.squeeze(np.where(ids == marker_ids[1]))\n",
    "    ref_pt2 = np.squeeze(corners[index[0]])[1]\n",
    "\n",
    "    # Lower right corner of ROI.\n",
    "    index = np.squeeze(np.where(ids == marker_ids[2]))\n",
    "    ref_pt3 = np.squeeze(corners[index[0]])[2]\n",
    "\n",
    "    # Lower left corner of ROI.\n",
    "    index = np.squeeze(np.where(ids == marker_ids[3]));\n",
    "    ref_pt4 = np.squeeze(corners[index[0]])[3]\n",
    "\n",
    "    return ref_pt1, ref_pt2, ref_pt3, ref_pt4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <font style=\"color:rgb(50,120,230)\">Convenience Function (scale destination points)<font/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scale_dst_points(ref_pt1, ref_pt2, ref_pt3, ref_pt4, scaling_fac_x = 0.01, scaling_fac_y = 0.01):\n",
    "\n",
    "    # Compute horizontal and vertical distance between markers.\n",
    "    x_distance = np.linalg.norm(ref_pt1 - ref_pt2) # distance between upper left and upper right markers.\n",
    "    y_distance = np.linalg.norm(ref_pt1 - ref_pt4) # distance between upper left and lower left markers.\n",
    "\n",
    "    delta_x = round(scaling_fac_x * x_distance)\n",
    "    delta_y = round(scaling_fac_y * y_distance)\n",
    "\n",
    "    # Apply the scaling factors to the ArUco Marker reference points to make\n",
    "    # the final adjustment for the destination points.\n",
    "    pts_dst = [[ref_pt1[0] - delta_x, ref_pt1[1] - delta_y]]\n",
    "    pts_dst = pts_dst + [[ref_pt2[0] + delta_x, ref_pt2[1] - delta_y]]\n",
    "    pts_dst = pts_dst + [[ref_pt3[0] + delta_x, ref_pt3[1] + delta_y]]\n",
    "    pts_dst = pts_dst + [[ref_pt4[0] - delta_x, ref_pt4[1] + delta_y]]\n",
    "\n",
    "    return pts_dst"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Initializations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "src_has_frame = True\n",
    "dst_has_frame = True\n",
    "frame_count = 0\n",
    "max_frames = 100\n",
    "color = (255,255,255)\n",
    "\n",
    "# Load the dictionary that was used to generate the markers.\n",
    "dictionary = cv2.aruco.getPredefinedDictionary(cv2.aruco.DICT_6X6_250)\n",
    "\n",
    "# Initialize the detector parameters using default values.\n",
    "#parameters = cv2.aruco.DetectorParameters_create()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. Process Source and Destination Frames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Processing frames ...')\n",
    "while src_has_frame & dst_has_frame:\n",
    "\n",
    "    if mime_dst == 'video':\n",
    "        # Get frame from the destination video.\n",
    "        dst_has_frame, frame_dst = cap_dst.read()\n",
    "        if not dst_has_frame:\n",
    "            break\n",
    "\n",
    "    if mime_src == 'video':\n",
    "        # The source is a video, so retrieve the source frame.\n",
    "        src_has_frame, frame_src = cap_src.read()\n",
    "        if not src_has_frame:\n",
    "            break\n",
    "\n",
    "    # Detect the markers in the image.\n",
    "    corners, ids, rejected = cv2.aruco.detectMarkers(frame_dst, dictionary)\n",
    "\n",
    "    # Extract reference point coordinates from marker corners.\n",
    "    ref_pt1, ref_pt2, ref_pt3, ref_pt4 = extract_pts(marker_ids, ids, corners)\n",
    "\n",
    "    # Scale destination points.\n",
    "    pts_dst = scale_dst_points(ref_pt1, ref_pt2, ref_pt3, ref_pt4, \n",
    "                                   scaling_fac_x = scaling_fac_x, \n",
    "                                   scaling_fac_y = scaling_fac_y)\n",
    "\n",
    "    # The source points are the four corners of the image source frame.\n",
    "    pts_src = [[0,0], [frame_src.shape[1], 0], [frame_src.shape[1], frame_src.shape[0]], [0, frame_src.shape[0]]]\n",
    "\n",
    "    # Convert list of points to arrays.\n",
    "    pts_src_m = np.asarray(pts_src)\n",
    "    pts_dst_m = np.asarray(pts_dst)\n",
    "\n",
    "    # Calculate the hmography.\n",
    "    h, mask = cv2.findHomography(pts_src_m, pts_dst_m, cv2.RANSAC)\n",
    "\n",
    "    # Warp source image onto the destination image.\n",
    "    warped_image = cv2.warpPerspective(frame_src, h, (frame_dst.shape[1], frame_dst.shape[0]))\n",
    "\n",
    "    # Prepare a mask representing the region to copy from the warped image into the destination frame.\n",
    "    mask = np.zeros([frame_dst.shape[0], frame_dst.shape[1]], dtype=np.uint8);\n",
    "     \n",
    "    # Fill ROI in destination frame with white to create mask.\n",
    "    cv2.fillConvexPoly(mask, np.int32([pts_dst_m]), (255, 255, 255), cv2.LINE_AA);\n",
    "\n",
    "    # Copy the mask into 3 channels.\n",
    "    warped_image = warped_image.astype(float)\n",
    "    mask3 = np.zeros_like(warped_image)\n",
    "    for i in range(0, 3):\n",
    "        mask3[:, :, i] = mask / 255\n",
    "    \n",
    "    # Create black region in destination frame ROI.\n",
    "    frame_masked = cv2.multiply(frame_dst.astype(float), 1 - mask3)\n",
    "    \n",
    "    # Create final result by adding warped image with the masked destination frame.\n",
    "    frame_out = cv2.add(warped_image, frame_masked)\n",
    "\n",
    "    # Showing the original frame and the new output frame side by side.\n",
    "    concatenated_output = cv2.hconcat([frame_dst.astype(float), frame_out])\n",
    "\n",
    "    # Draw a white vertical line that divides the two image frames.\n",
    "    frame_w = concatenated_output.shape[1]\n",
    "    frame_h = concatenated_output.shape[0]\n",
    "    concatenated_output = cv2.line(concatenated_output, \n",
    "                                   (int(frame_w / 2), 0), \n",
    "                                   (int(frame_w / 2), frame_h), \n",
    "                                   color, thickness = 8)\n",
    "\n",
    "    # Create output file.\n",
    "    if (mime_dst == 'image' and mime_src == 'image'):\n",
    "        # Create output image.\n",
    "        cv2.imwrite(output_file, concatenated_output.astype(np.uint8))\n",
    "        break\n",
    "    else:\n",
    "        # Create output video.\n",
    "        video_writer.write(concatenated_output.astype(np.uint8))\n",
    "        \n",
    "cv2.destroyAllWindows()\n",
    "if 'video_writer' in locals():\n",
    "    video_writer.release()\n",
    "    print('Processing complete, video writer released ...')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6. Display Results\n",
    "\n",
    "Each of the cases shown below were generated by executing the above cells for each of the four defined cases. The execution of each case generates an output file (image or video) and the cells below load the output files in the notebook for display."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <font style=\"color:rgb(50,120,230)\">Case 4: Video in Video</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clip = VideoFileClip('AR_video_in_video_horse_race.mp4')\n",
    "clip.display_in_notebook(width = 1000)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "opencv-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
